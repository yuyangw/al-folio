<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Yuyang  Wang


  | Publications

</title>
<meta name="description" content="A beautiful, simple, clean, and responsive Jekyll theme for academics">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<!-- 
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>">
 -->
<!-- Add CMU favicon -->
<link rel="shortcut icon" type="image/jpg" href="../assets/img/favicon-cmu.png">
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://yuyangw.github.io/">
       <span class="font-weight-bold">Yuyang</span>   Wang
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          <!--  -->
          <!-- Blog -->
          <!-- <li class="nav-item "> -->
            <!-- <a class="nav-link" href="/blog/"> -->
              <!-- blog -->
              <!--  -->
            <!-- </a> -->
          <!-- </li> -->
          
          <!-- Other pages -->
          <!--  -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                Publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <div class="publications">

My full list of publications can be found on <a href="https://scholar.google.com/citations?user=6eWGKEsAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Google Scholar</a> page.


  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2023mcf.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2023mcf.png" style="width:200px;"> -->
    
  </div>

  <div id="wang2023generating" class="col-sm-8">
    
      <div class="title">Generating Molecular Conformer Fields</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Elhag, Ahmed, -->
                  Ahmed Elhag,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Jaitly, Navdeep, -->
                  Navdeep Jaitly,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Susskind, Joshua, -->
                  Joshua Susskind,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Bautista, Miguel Angel -->
                  and Miguel Angel Bautista
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>GenBio Workshop at NeurIPS</em>
      
      
        2023
      
      
      </div>
    

    <div class="links">
    
    
      <a href="http://arxiv.org/abs/2311.17932" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2311.17932.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper we tackle the problem of generating conformers of a molecule in 3D space given its molecular graph. We parameterize these conformers as continuous functions that map elements from the molecular graph to points in 3D space. We then formulate the problem of learning to generate conformers as learning a distribution over these functions using a diffusion generative model, called Molecular Conformer Fields (MCF). Our approach is simple and scalable, and achieves state-of-the-art performance on challenging molecular conformer generation benchmarks while making no assumptions about the explicit structure of molecules (e.g. modeling torsional angles). MCF represents an advance in extending diffusion models to handle complex scientific problems in a conceptually simple, scalable and effective manner.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2023mdf.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2023mdf.png" style="width:200px;"> -->
    
  </div>

  <div id="elhag2023manifold" class="col-sm-8">
    
      <div class="title">Manifold Diffusion Fields</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Elhag, Ahmed A, -->
                  Ahmed A Elhag,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Susskind, Joshua M, -->
                  Joshua M Susskind,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Bautista, Miguel Angel -->
                  and Miguel Angel Bautista
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Diffusion Models Workshop at NeurIPS</em>
      
      
        2023
      
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="https://openreview.net/pdf?id=0AMMdKcuae" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present Manifold Diffusion Fields (MDF), an approach that unlocks learning of diffusion models of data in general non-Euclidean geometries. Leveraging insights from spectral geometry analysis, we define an intrinsic coordinate system on the manifold via the eigen-functions of the Laplace-Beltrami Operator. MDF represents functions using an explicit parametrization formed by a set of multiple input-output pairs. Empirical results on multiple datasets and manifolds including challenging scientific problems like weather prediction or molecular conformation show that MDF can capture distributions of such functions with better diversity and fidelity than previous approaches.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2023denoise.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2023denoise.png" style="width:200px;"> -->
    
  </div>

  <div id="wang2023denoise" class="col-sm-8">
    
      <div class="title">Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Xu, Changwen, -->
                  Changwen Xu,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Li, Zijie, -->
                  Zijie Li,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Chemical Theory and Computation</em>
      
      
        2023
      
      
      </div>
    

    <div class="links">
    
      <a href="https://pubs.acs.org/doi/10.1021/acs.jctc.3c00289" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/2303.02216" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://pubs.acs.org/doi/epdf/10.1021/acs.jctc.3c00289" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/yuyangw/Denoise-Pretrain-ML-Potential" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent advances in equivariant graph neural networks (GNNs) have made deep learning amenable to developing fast surrogate models to expensive ab initio quantum mechanics (QM) approaches for molecular potential predictions. However, building accurate and transferable potential models using GNNs remains challenging, as the data is greatly limited by the expensive computational costs and level of theory of QM methods, especially for large and complex molecular systems. In this work, we propose denoise pretraining on nonequilibrium molecular conformations to achieve more accurate and transferable GNN potential predictions. Specifically, atomic coordinates of sampled nonequilibrium conformations are perturbed by random noises and GNNs are pretrained to denoise the perturbed molecular conformations which recovers the original coordinates. Rigorous experiments on multiple benchmarks reveal that pretraining significantly improves the accuracy of neural potentials. Furthermore, we show that the proposed pretraining approach is model-agnostic, as it improves the performance of different invariant and equivariant GNNs. Notably, our models pretrained on small molecules demonstrate remarkable transferability, improving performance when fine-tuned on diverse molecular systems, including different elements, charged molecules, biomolecules, and larger systems. These results highlight the potential for leveraging denoise pretraining approaches to build more generalizable neural potentials for complex molecular systems.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2023neural.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2023neural.png" style="width:200px;"> -->
    
  </div>

  <div id="cao2023neural" class="col-sm-8">
    
      <div class="title">Neural Network Predicts Ion Concentration Profiles under Nanoconfinement</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Cao, Zhonglin, -->
                  Zhonglin Cao,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Lorsung, Cooper, -->
                  Cooper Lorsung,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Chemical Physics</em>
      
      
        2023
      
      
      </div>
    

    <div class="links">
    
      <a href="https://pubs.aip.org/aip/jcp/article-abstract/159/9/094702/2908717/Neural-network-predicts-ion-concentration-profiles?redirectedFrom=fulltext" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/2304.04896" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2304.04896.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/zcao0420/IonNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Modeling the ion concentration profile in nanochannel plays an important role in understanding the electrical double layer and electroosmotic flow. Due to the non-negligible surface interaction and the effect of discrete solvent molecules, molecular dynamics (MD) simulation is often used as an essential tool to study the behavior of ions under nanoconfinement. Despite the accuracy of MD simulation in modeling nanoconfinement systems, it is computationally expensive. In this work, we propose neural network to predict ion concentration profiles in nanochannels with different configurations, including channel widths, ion molarity, and ion types. By modeling the ion concentration profile as a probability distribution, our neural network can serve as a much faster surrogate model for MD simulation with high accuracy. We further demonstrate the superior prediction accuracy of neural network over XGBoost. Lastly, we demonstrated that neural network is flexible in predicting ion concentration profiles with different bin sizes. Overall, our deep learning model is a fast, flexible, and accurate surrogate model to predict ion concentration profiles in nanoconfinement.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2023transpolymer.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2023transpolymer.png" style="width:200px;"> -->
    
  </div>

  <div id="xu2023transpolymer" class="col-sm-8">
    
      <div class="title">TransPolymer: a Transformer-based Language Model for Polymer Property Predictions</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Xu, Changwen, -->
                  Changwen Xu,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>npj Computational Materials</em>
      
      
        2023
      
      
      </div>
    

    <div class="links">
    
      <a href="https://www.nature.com/articles/s41524-023-01016-5" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/2209.01307" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://www.nature.com/articles/s41524-023-01016-5.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/ChangwenXu98/TransPolymer" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Accurate and efficient prediction of polymer properties is of great significance in polymer design. Conventionally, expensive and time-consuming experiments or simulations are required to evaluate polymer functions. Recently, Transformer models, equipped with self-attention mechanisms, have exhibited superior performance in natural language processing. However, such methods have not been investigated in polymer sciences. Herein, we report TransPolymer, a Transformer-based language model for polymer property prediction. Our proposed polymer tokenizer with chemical awareness enables learning representations from polymer sequences. Rigorous experiments on ten polymer property prediction benchmarks demonstrate the superior performance of TransPolymer. Moreover, we show that TransPolymer benefits from pretraining on large unlabeled dataset via Masked Language Modeling. Experimental results further manifest the important role of self-attention in modeling polymer sequences. We highlight this model as a promising computational tool for promoting rational polymer design and understanding structure-property relationships from a data science view.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2023moformer.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2023moformer.png" style="width:200px;"> -->
    
  </div>

  <div id="cao2023moformer" class="col-sm-8">
    
      <div class="title">MOFormer: Self-Supervised Transformer Model for Metal-Organic Framework Property Prediction</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Cao, Zhonglin, -->
                  Zhonglin Cao,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Magar, Rishikesh, -->
                  Rishikesh Magar,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of the American Chemical Society</em>
      
      
        2023
      
      
      </div>
    

    <div class="links">
    
      <a href="https://pubs.acs.org/doi/10.1021/jacs.2c11420" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/2210.14188" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://pubs.acs.org/doi/pdf/10.1021/jacs.2c11420" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/zcao0420/MOFormer" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Metalâ€“organic frameworks (MOFs) are materials with a high degree of porosity that can be used for many applications. However, the chemical space of MOFs is enormous due to the large variety of possible combinations of building blocks and topology. Discovering the optimal MOFs for specific applications requires an efficient and accurate search over countless potential candidates. Previous high-throughput screening methods using computational simulations like DFT can be time-consuming. Such methods also require the 3D atomic structures of MOFs, which adds one extra step when evaluating hypothetical MOFs. In this work, we propose a structure-agnostic deep learning method based on the Transformer model, named as MOFormer, for property predictions of MOFs. MOFormer takes a text string representation of MOF (MOFid) as input, thus circumventing the need of obtaining the 3D structure of a hypothetical MOF and accelerating the screening process. By comparing to other descriptors such as Stoichiometric-120 and revised autocorrelations, we demonstrate that MOFormer can achieve state-of-the-art structure-agnostic prediction accuracy on all benchmarks. Furthermore, we introduce a self-supervised learning framework that pretrains the MOFormer via maximizing the cross-correlation between its structure-agnostic representations and structure-based representations of the crystal graph convolutional neural network (CGCNN) on &gt;400k publicly available MOF data. Benchmarks show that pretraining improves the prediction accuracy of both models on various downstream prediction tasks. Furthermore, we revealed that MOFormer can be more data-efficient on quantum-chemical property prediction than structure-based CGCNN when training data is limited. Overall, MOFormer provides a novel perspective on efficient MOF property prediction using deep learning.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2022GNN.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2022GNN.png" style="width:200px;"> -->
    
  </div>

  <div id="wang2022GNN" class="col-sm-8">
    
      <div class="title">Graph Neural Networks for Molecules</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Li, Zijie, -->
                  Zijie Li,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>A chapter for book "Machine Learning in Molecular Sciences" (Editor: Dr. Jerzy Leszczynski) published by Springer Nature</em>
      
      
        2023
      
      
      </div>
    

    <div class="links">
    
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-37196-7_2" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/2209.05582" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2209.05582.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Graph neural networks (GNNs), which are capable of learning representations from graphical data, are naturally suitable for modeling molecular systems. This review introduces GNNs and their various applications for small organic molecules. GNNs rely on message-passing operations, a generic yet powerful framework, to update node features iteratively. Many researches design GNN architectures to effectively learn topological information of 2D molecule graphs as well as geometric information of 3D molecular systems. GNNs have been implemented in a wide variety of molecular applications, including molecular property prediction, molecular scoring and docking, molecular optimization and de novo generation, molecular dynamics simulation, etc. Besides, the review also summarizes the recent development of self-supervised learning for molecules with GNNs.</p>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2022IL.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2022IL.png" style="width:200px;"> -->
    
  </div>

  <div id="jian2022predicting" class="col-sm-8">
    
      <div class="title">Predicting CO<sub>2</sub> Absorption in Ionic Liquids with Molecular Descriptors and Explainable Graph Neural Networks</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Jian, Yue, -->
                  Yue Jian,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>ACS Sustainable Chemistry &amp; Engineering</em>
      
      
        2022
      
      
      </div>
    

    <div class="links">
    
      <a href="https://pubs.acs.org/doi/full/10.1021/acssuschemeng.2c05985" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/https://arxiv.org/abs/2210.01120" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2210.01120.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/ftyuejian/Predicting-CO2-Absorption-in-Ionic-Liquid-with-Molecular-Descriptors-and-Explainable-GNN" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Ionic Liquids (ILs) provide a promising solution for CO2 capture and storage to mitigate global warming. However, identifying and designing the high-capacity IL from the giant chemical space requires expensive, and exhaustive simulations and experiments. Machine learning (ML) can accelerate the process of searching for desirable ionic molecules through accurate and efficient property predictions in a data-driven manner. But existing descriptors and ML models for the ionic molecule suffer from the inefficient adaptation of molecular graph structure. Besides, few works have investigated the explainability of ML models to help understand the learned features that can guide the design of efficient ionic molecules. In this work, we develop both fingerprint-based ML models and Graph Neural Networks (GNNs) to predict the CO2 absorption in ILs. Fingerprint works on graph structure at the feature extraction stage, while GNNs directly handle molecule structure in both the feature extraction and model prediction stage. We show that our method outperforms previous ML models by reaching a high accuracy (MAE of 0.0137, R2 of 0.9884). Furthermore, we take the advantage of GNNs feature representation and develop a substructure-based explanation method that provides insight into how each chemical fragments within IL molecules contribute to the CO2 absorption prediction of ML models. We also show that our explanation result agrees with some ground truth from the theoretical reaction mechanism of CO2 absorption in ILs, which can advise on the design of novel and efficient functional ILs in the future.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2022crystaltwins.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2022crystaltwins.png" style="width:200px;"> -->
    
  </div>

  <div id="magar2022crystal" class="col-sm-8">
    
      <div class="title">Crystal Twins: Self-supervised Learning for Crystalline Material Property Prediction</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Magar, Rishikesh, -->
                  Rishikesh Magar,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>npj Computational Materials</em>
      
      
        2022
      
      
      </div>
    

    <div class="links">
    
      <a href="https://www.nature.com/articles/s41524-022-00921-5" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/2205.01893" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2205.01893.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/RishikeshMagar/Crystal-Twins" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Machine learning (ML) models have been widely successful in the prediction of material properties. However, large labeled datasets required for training accurate ML models are elusive and computationally expensive to generate. Recent advances in Self-Supervised Learning (SSL) frameworks capable of training ML models on unlabeled data have mitigated this problem and demonstrated superior performance in computer vision and natural language processing tasks. Drawing inspiration from the developments in SSL, we introduce Crystal Twins (CT): an SSL method for crystalline materials property prediction. Using a large unlabeled dataset, we pre-train a Graph Neural Network (GNN) by applying the redundancy reduction principle to the graph latent embeddings of augmented instances obtained from the same crystalline system. By sharing the pre-trained weights when fine-tuning the GNN for regression tasks, we significantly improve the performance for 7 challenging material property prediction benchmarks.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2022imolclr.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2022imolclr.png" style="width:200px;"> -->
    
  </div>

  <div id="wang2022improving" class="col-sm-8">
    
      <div class="title">Improving Molecular Contrastive Learning via Faulty Negative Mitigation and Decomposed Fragment Contrast</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Magar, Rishikesh, -->
                  Rishikesh Magar,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Liang, Chen, -->
                  Chen Liang,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Chemical Information and Modeling</em>
      
      
        2022
      
      
      </div>
    

    <div class="links">
    
      <a href="https://pubs.acs.org/doi/full/10.1021/acs.jcim.2c00495" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/2202.09346" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2202.09346.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/yuyangw/iMolCLR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep learning has been a prevalence in computational chemistry and widely implemented in molecule property predictions. Recently, self-supervised learning (SSL), especially contrastive learning (CL), gathers growing attention for the potential to learn molecular representations that generalize to the gigantic chemical space. Unlike supervised learning, SSL can directly leverage large unlabeled data, which greatly reduces the effort to acquire molecular property labels through costly and time-consuming simulations or experiments. However, most molecular SSL methods borrow the insights from the machine learning community but neglect the unique cheminformatics (e.g., molecular fingerprints) and multi-level graphical structures (e.g., functional groups) of molecules. In this work, we propose iMolCLR: improvement of Molecular Contrastive Learning of Representations with graph neural networks (GNNs) in two aspects, (1) mitigating faulty negative contrastive instances via considering cheminformatics similarities between molecule pairs; (2) fragment-level contrasting between intra- and inter-molecule substructures decomposed from molecules. Experiments have shown that the proposed strategies significantly improve the performance of GNN models on various challenging molecular property predictions. In comparison to the previous CL framework, iMolCLR demonstrates an averaged 1.2% improvement of ROC-AUC on 8 classification benchmarks and an averaged 10.1% decrease of the error on 6 regression benchmarks. On most benchmarks, the generic GNN pre-trained by iMolCLR rivals or even surpasses supervised learning models with sophisticated architecture designs and engineered features. Further investigations demonstrate that representations learned through iMolCLR intrinsically embed scaffolds and functional groups that can reason molecule similarities.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2022molclr.gif" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2022molclr.gif" style="width:200px;"> -->
    
  </div>

  <div id="wang2022molclr" class="col-sm-8">
    
      <div class="title">Molecular Contrastive Learning of Representations via Graph Neural Networks</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Wang, Jianren, -->
                  Jianren Wang,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Cao, Zhonglin, -->
                  Zhonglin Cao,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Nature Machine Intelligence</em>
      
      
        2022
      
      
      </div>
    

    <div class="links">
    
      <a href="https://www.nature.com/articles/s42256-022-00447-x" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/2102.10056" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://www.nature.com/articles/s42256-022-00447-x.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/yuyangw/MolCLR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      <a href="https://techxplore.com/news/2022-03-machine-smarter-drug-discovery.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Media</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Molecular machine learning bears promise for efficient molecular property prediction and drug discovery. However, labelled molecule data can be expensive and time consuming to acquire. Due to the limited labelled data, it is a great challenge for supervised-learning machine learning models to generalize to the giant chemical space. Here we present MolCLR (Molecular Contrastive Learning of Representations via Graph Neural Networks), a self-supervised learning framework that leverages large unlabelled data (Â 10 million unique molecules). In MolCLR pre-training, we build molecule graphs and develop graph-neural-network encoders to learn differentiable representations. Three molecule graph augmentations are proposed: atom masking, bond deletion and subgraph removal. A contrastive estimator maximizes the agreement of augmentations from the same molecule while minimizing the agreement of different molecules. Experiments show that our contrastive learning framework significantly improves the performance of graph-neural-network encoders on various molecular property benchmarks including both classification and regression tasks. Benefiting from pre-training on the large unlabelled database, MolCLR even achieves state of the art on several challenging benchmarks after fine-tuning. In addition, further investigations demonstrate that MolCLR learns to embed molecules into representations that can distinguish chemically reasonable molecular similarities.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2022gpcr.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2022gpcr.png" style="width:200px;"> -->
    
  </div>

  <div id="yadav2022prediction" class="col-sm-8">
    
      <div class="title">Prediction of GPCR activity using Deep Learning</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Yadav, Prakarsh, -->
                  Prakarsh Yadav,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Mollaei, Parisa, -->
                  Parisa Mollaei,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Cao, Zhonglin, -->
                  Zhonglin Cao,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Computational and Structural Biotechnology Journal</em>
      
      
        2022
      
      
      </div>
    

    <div class="links">
    
      <a href="https://www.sciencedirect.com/science/article/pii/S2001037022001726?via%3Dihub" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
    
    
      
      <a href="/assets/pdf/Prediction_of_GPCR_activity_using_Deep_Learning.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>GPCRs are the target for one-third of the FDA-approved drugs, however; the development of new drug molecules targeting GPCRs is limited by the lack of mechanistic understanding of the GPCR structure-activity-function relationship. To modulate the GPCR activity with highly specific drugs and minimal side-effects, it is necessary to quantitatively describe the important structural features in the GPCR and correlate them to the activation state of GPCR. In this study, we developed 3 ML approaches to predict the conformation state of GPCR proteins. Additionally, we predict the activity level of GPCRs based on their structure. We leverage the unique advantages of each of the 3 ML approach, interpretability of XGBoost, minimal feature engineering for 3D convolutional neural network, and graph representation of protein structure for graph neural network. By using these ML approaches, we are able to predict the GPCRs activation state with high accuracy (91%-95%) and also predict the activation state of GPCRs with low error (MAE of 7.15-10.58). Furthermore, the interpretation of the ML approaches allow us to determine the importance of each of the features in distinguishing between the GPCRs conformations.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2021auglichem.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2021auglichem.png" style="width:200px;"> -->
    
  </div>

  <div id="magar2021auglichem" class="col-sm-8">
    
      <div class="title">AugLiChem: Data Augmentation Library of Chemical Structures for Machine Learning</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Magar*, Rishikesh, -->
                  Rishikesh Magar*,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang*</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang*</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Lorsung*, Cooper, -->
                  Cooper Lorsung*,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Liang, Chen, -->
                  Chen Liang,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Ramasubramanian, Hariharan, -->
                  Hariharan Ramasubramanian,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Li, Peiyuan, -->
                  Peiyuan Li,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Machine Learning: Science and Technology</em>
      
      
        2022
      
      
      </div>
    

    <div class="links">
    
      <a href="https://iopscience.iop.org/article/10.1088/2632-2153/ac9c84" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/2111.15112" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://arxiv.org/pdf/2111.15112.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/BaratiLab/AugLiChem" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Machine learning (ML) has demonstrated the promise for accurate and efficient property prediction of molecules and crystalline materials. To develop highly accurate ML models for chemical structure property prediction, datasets with sufficient samples are required. However, obtaining clean and sufficient data of chemical properties can be expensive and time-consuming, which greatly limits the performance of ML models. Inspired by the success of data augmentations in computer vision and natural language processing, we developed AugLiChem: the data augmentation library for chemical structures. Augmentation methods for both crystalline systems and molecules are introduced, which can be utilized for fingerprint-based ML models and Graph Neural Networks (GNNs). We  show that using our augmentation strategies significantly improves the performance of ML models, especially when using GNNs. In addition, the augmentations that we developed can be used as a direct plug-in module during training and have demonstrated the effectiveness when implemented with different GNN models through the AugliChem library. The Python-based package for our implementation of Auglichem: Data augmentation library for chemical structures, is publicly available at: https://github.com/BaratiLab/AugLiChem.</p>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2021catgym.jpg" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2021catgym.jpg" style="width:200px;"> -->
    
  </div>

  <div id="yoon2021deep" class="col-sm-8">
    
      <div class="title">Deep Reinforcement Learning for Predicting Kinetic Pathways to Surface Reconstruction in a Ternary Alloy.</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Yoon, Junwoong, -->
                  Junwoong Yoon,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Cao, Zhonglin, -->
                  Zhonglin Cao,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Raju, Rajesh, -->
                  Rajesh Raju,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Burnley, Robert, -->
                  Robert Burnley,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Gellman, Andrew J, -->
                  Andrew J Gellman,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Barati Farimani, Amir, -->
                  Amir Barati Farimani,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Ulissi, Zachary W -->
                  and Zachary W Ulissi
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Machine Learning: Science and Technology</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a href="https://iopscience.iop.org/article/10.1088/2632-2153/ac191c" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
    
    
      
      <a href="https://iopscience.iop.org/article/10.1088/2632-2153/ac191c/pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/ulissigroup/catgym" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      <a href="https://www.marktechpost.com/2021/10/03/cmu-researchers-introduce-catgym-a-deep-reinforcement-learning-drl-environment-for-predicting-kinetic-pathways-to-surface-reconstruction-in-a-ternary-alloy/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Media</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The majority of computational catalyst design focuses on the screening of material components and alloy composition to optimize selectivity and activity for a given reaction. However, predicting the metastability of the alloy catalyst surface at realistic operating conditions requires an extensive sampling of possible surface reconstructions and their associated kinetic pathways. We present CatGym, a deep reinforcement learning (DRL) environment for predicting the thermal surface reconstruction pathways and their associated kinetic barriers in crystalline solids under reaction conditions. The DRL agent iteratively changes the positions of atoms in the near-surface region to generate kinetic pathways to accessible local minima involving changes in the surface compositions. We showcase our agent by predicting the surface reconstruction pathways of a ternary Ni3Pd3Au2(111) alloy catalyst. Our results show that the DRL agent can not only explore more diverse surface compositions than the conventional minima hopping method, but also generate the kinetic surface reconstruction pathways. We further demonstrate that the kinetic pathway to a global minimum energy surface composition and its associated transition state predicted by our agent is in good agreement with the minimum energy path predicted by nudged elastic band calculations.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2021grapheneRL.gif" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2021grapheneRL.gif" style="width:200px;"> -->
    
  </div>

  <div id="wang2021efficient" class="col-sm-8">
    
      <div class="title">Efficient Water Desalination with Graphene Nanopores Obtained using Artificial Intelligence</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- <b><i>Wang*</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang*</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Cao*, Zhonglin, -->
                  Zhonglin Cao*,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>npj 2D Materials and Applications</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a href="https://www.nature.com/articles/s41699-021-00246-9" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
      <a href="http://arxiv.org/abs/2101.07399" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
      
      <a href="https://www.nature.com/articles/s41699-021-00246-9.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
      <a href="https://github.com/BaratiLab/Graphene-RL" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      <a href="https://engineering.cmu.edu/news-events/news/2021/10/20-ai-desalination.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Media</a>
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Two-dimensional nanomaterials, such as graphene, have been extensively studied because of their outstanding physical properties. Structure and topology of nanopores on such materials can be important for their performances in real-world engineering applications, like water desalination. However, discovering the most efficient nanopores often involves a very large number of experiments or simulations that are expensive and time-consuming. In this work, we propose a data-driven artificial intelligence (AI) framework for discovering the most efficient graphene nanopore for water desalination. Via a combination of deep reinforcement learning (DRL) and convolutional neural network (CNN), we are able to rapidly create and screen thousands of graphene nanopores and select the most energy-efficient ones. Molecular dynamics (MD) simulations on promising AI-created graphene nanopores show that they have higher water flux while maintaining rival ion rejection rate compared to the normal circular nanopores. Irregular shape with rough edges geometry of AI-created pores is found to be the key factor for their high water desalination performance. Ultimately, this study shows that AI can be a powerful tool for nanomaterial design and screening.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2021aril.gif" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2021aril.gif" style="width:200px;"> -->
    
  </div>

  <div id="wang2021aril" class="col-sm-8">
    
      <div class="title">Adversarially Robust Imitation Learning</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Wang, Jianren, -->
                  Jianren Wang,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Zhuang, Ziwen, -->
                  Ziwen Zhuang,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Zhao, Hang -->
                  and Hang Zhao
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 5th Annual Conference on Robot Learning</em>
      
      
        2021
      
      
      </div>
    

    <div class="links">
    
      <a href="https://openreview.net/forum?id=9aVCUv3nKBg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
    
    
    
    
      
      <a href="https://openreview.net/pdf?id=9aVCUv3nKBg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Modern imitation learning (IL) utilizes deep neural networks (DNNs) as function approximators to mimic the policy of the expert demonstrations. However, DNNs can be easily fooled by subtle noise added to the input, which is even non-detectable by humans. This makes the learned agent vulnerable to attacks, especially in IL where agents can struggle to recover from the errors. In such light, we propose a sound Adversarially Robust Imitation Learning (ARIL) method. In our setting, an agent and an adversary are trained alternatively. The former with adversarially attacked input at each timestep mimics the behavior of an online expert and the latter learns to add perturbations on the states by forcing the learned agent to fail on choosing the right decisions. We theoretically prove that ARIL can achieve adversarial robustness and evaluate ARIL on multiple benchmarks from DM Control Suite. The result reveals that our method (ARIL) achieves better robustness compare with other imitation learning methods under both sensory attack and physical attack.</p>
    </div>
    
  </div>
</div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2020peptidegan.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2020peptidegan.png" style="width:200px;"> -->
    
  </div>

  <div id="wang2020bio" class="col-sm-8">
    
      <div class="title">Bio-informed Protein Sequence Generation for Multi-class Virus Mutation Prediction</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Yadav, Prakarsh, -->
                  Prakarsh Yadav,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Magar, Rishikesh, -->
                  Rishikesh Magar,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>bioRxiv</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://www.biorxiv.org/content/10.1101/2020.06.11.146167v1.abstract" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">biorxiv</a>
    
    
      
      <a href="https://www.biorxiv.org/content/biorxiv/early/2020/06/12/2020.06.11.146167.full.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Viral pandemics are emerging as a serious global threat to public health, like the recent outbreak of COVID-19. Viruses, especially those belonging to a large family of +ssRNA viruses, have a high possibility of mutating by inserting, deleting, or substituting one or multiple genome segments. It is of great importance for human health worldwide to predict the possible virus mutations, which can effectively avoid the potential second outbreak. In this work, we develop a GAN-based multi-class protein sequence generative model, named ProteinSeqGAN. Given the viral species, the generator is modeled on RNNs to predict the corresponding antigen epitope sequences synthesized by viral genomes. Additionally, a Graphical Protein Autoencoder (GProAE) built upon VAE is proposed to featurize proteins bioinformatically. GProAE, as a multi-class discriminator, also learns to evaluate the goodness of protein sequences and predict the corresponding viral species. Further experiments show that our ProteinSeqGAN model can generate valid antigen protein sequences from both bioinformatics and statistics perspectives, which can be promising predictions of virus mutations.</p>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid" src="/assets/pubimg/2020emsr.png" style="width:200px;">
    <!-- <img class="img-fluid" src="../../assets/pubimg/2020emsr.png" style="width:200px;"> -->
    
  </div>

  <div id="mullicklearning" class="col-sm-8">
    
      <div class="title">Learning Super-Resolution Electron Density Map of Proteins using 3D U-Net</div>
      <div class="author">
        
          
          
          
            
              
                
                  <!-- Mullick, Baishali, -->
                  Baishali Mullick,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- <b><i>Wang</i></b>, <b><i>Yuyang</i></b>, -->
                  <b><i>Yuyang</i></b> <b><i>Wang</i></b>,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- Yadav, Prakarsh, -->
                  Prakarsh Yadav,
                
              
            
          
        
          
          
          
            
              
                
                  <!-- and Barati Farimani, Amir -->
                  and Amir Barati Farimani
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Machine Learning for Structural Biology Workshop at NeurIPS</em>
      
      
        2020
      
      
      </div>
    

    <div class="links">
    
    
    
    
      
      <a href="https://www.mlsb.io/papers/MLSB2020_Learning_Super-Resolution_Electron_Density.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>A well-established protein structure is essential for understanding protein molecular mechanism, phenotypic implication and drug discovery. Recent development of cryo-Electron Microscopy (cryo-EM) offers the advantage of easy sample preparation and not requiring crystallized protein for structural biology. However, the resolution of cryo-EM electron density maps used to determine protein structure, is not at par with X-ray diffraction (XRD) or NMR. In this work, we propose to leverage a deep learning-based model to increase the resolution of low-quality electron density maps. The model is built upon U-Net with 3D convolutional layers, which contains three components: encoder, bottleneck, and decoder. To get paired maps of different resolutions, we collect high-resolution maps from XRD as ground truth labels. While the low-resolution maps are obtained through a noise model which combines dilation operations, Gaussian filters and Gaussian noise. We also introduce data augmentation techniques during model training, like random cropping, rotation, and flipping. Experiments show that when applied to low-resolution electron maps, the U-Net model can improve the resolution in the metric of EMRinger score, which redesigns the map so that it resolves the regions of ambiguity to offer greater certainty in the position of amino acids. </p>
    </div>
    
  </div>
</div>
</li>
</ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    <!-- 
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2023 Yuyang  Wang.
    
    
    
  </div>
</footer>
 -->


  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
